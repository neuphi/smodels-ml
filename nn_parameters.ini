#Database address
[pathing]
databasePath = ../../smodels-database ;Give path to the database "text" directory or to the binary database file -LLP
smodelsPath  = ../../smodels
utilsPath    = ../../smodels-utils
outputPath   = database ;Directory of neural network database for finished models and their analyses, performances, loss plots and logs. If left blank, default database folder will be chosen

#Select database analyses
[database]
analysis = ATLAS-SUSY-2016-15 ;Select single analysis for training CMS-SUS-19-006
txName = T2tt ;Select topologies you want to train, eg: 'T2tt' or 'T2tt,T6WW,..' or 'all'  THSCPM1b 
dataselector = upperLimit ;upperLimit ;efficiencyMap ;Select upperLimit or efficiencyMap results
;signalRegion = SR1FULL_175 ;Select SR if dataselector == efficiencyMap SR1FULL_825
overwrite = always ;Check whether you want to override an existing NN with new results. Options include: always, never, outperforming


#General dataset options
[dataset]
samplesize = 1000, 1000 ;Size of total training data for regression and classification respectively
;externalFile = maps_jan/sample_updated/THSCPM1b_eff_mutrig_v3.dat ;HSCP_ATLAS_1902_01636_eff_maps_Oct5_sfgrid/THSCPM8_eff_mutrig.dat ;Load custom generated dataset. Requires full relative path. Ignores samplesize and instead splits the whole file into traning, testing and validation sets.
;massColumns = 0,0,-2,-2,7 ;Mass columns of loaded file. Use SmodelS type input in form of [b1_mass0,b1_mass1,b1_mass2,b2_mass0,b2_mass1,b2_mass2,b1_width,b2_width,eff/ul], NEGATIVE INDICES MEAN WIDTHS AND WILL BE RESCALED TO LOG(1+w) - M1b,2b 0,0,-2,-2,7 - M5 2,1,0,2,1,0,-4,-4,9 - M8 1,0,1,0,-3,-3,8
sampleSplit = 0.8,0.1,0.1 ;Set the ratio our sample data will be split into training, testing and validation sets
;refXsecFile = ../slha/xsecsquark13.txt ;reference xsec file vom utils/slha folder. (optional). Used to calculate a limit of [lumi * refXsec(m0) * eff > 0.01] where all efficiencies will be set to zero. Useful for sparing the NN the need of learning unneccessarily small effs.
;refXsecColumns = 0,2 ;columns of masses and xsecs of refXsec file


#Computation device options
[computation]
device = cpu ;Specify which device to be used for computing, 'cpu' for CPU, int 'n' for gpu:'n', default = cpu 
cores = 6 ; only works for device = cpu


#Hyper-parameters for regression networks
[regression]
optimizer = Adam
lossFunction = RMSE ;currently implemented options are MSE (mean squared error) and RMSE (relative MSE)
learnRate = 1e-3 ;1e-3 seems best for RMSE
batchSize = 16 ; 16
epochNum = 60 ; 80
shape = trap ;Specify shape/architecture of NN (more detailed explanation coming)
activationFunction = prel ;Set activation functions used in hidden layers   LLP note: sel ~80% err, prel ~40%, rel ~30%
layer = 2 ;Set # of hidden layers
nodes = 128 ; 256 ;Set maximum nodes in largest hidden layer
rescaleMethodMasses = minmaxScaler ; Rescaling method for input masses (options: standardScore, minmaxScaler)
;rescaleMethodTargets = log ; Rescaling method for target values (options: standardScore, boxcox, log)


#Hyper-parameters for classification networks
[classification]
optimizer = Adam
lossFunction = BCE ;binary cross entropy seems to be the best option here
learnRate = 1e-3
batchSize = 16
epochNum = 4
shape = trap ;Specify shape/architecture of NN (more detailed explanation coming) lin?
activationFunction = prel ;Set activation functions used in hidden layers. Options: rel, prel, sel, lrel
layer = 2 ;Set # of hidden layers
nodes = 128 ;Set maximum nodes in largest hidden layer
rescaleMethodMasses = minmaxScaler ; Rescaling method for input masses (options: standardScore, minmaxScaler)

#Plots and analysis
[validation]
logFile = false ; create log file of losses and parameters of all models for each topology
lossPlot = true ; create loss plot of final models
runPerformance = false ;run evaluatePerformance.py on final models after search has been completed
